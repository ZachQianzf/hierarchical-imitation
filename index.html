
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-6');
</script>
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="resources/seal_icon.png">
  <title>Third-Person Visual Imitation Learning Decoupled Hierarchical Controller</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://pathak22.github.io/hierarchical-imitation/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Hierarchical Third-Person Imitation Learning" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Third-Person Visual Imitation Learning Decoupled Hierarchical Controller" />
  <meta property="og:description" content="Sharma, Pathak, Gupta. NeurIPS 2019. Third-Person Visual Imitation Learning Decoupled Hierarchical Controller." />
  <meta property="og:url" content="https://pathak22.github.io/hierarchical-imitation/" />
  <meta property="og:image" content="https://pathak22.github.io/hierarchical-imitation/resources/teaser.jpg" />
  <meta property="og:video" content="https://www.youtube.com/v/eWBkDuNFEKA" />

  <meta property="article:publisher" content="http://people.eecs.berkeley.edu/~pathak/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Third-Person Visual Imitation Learning Decoupled Hierarchical Controller" />
  <meta name="twitter:description" content="Sharma, Pathak, Gupta. NeurIPS 2019. Third-Person Visual Imitation Learning Decoupled Hierarchical Controller." />
  <meta name="twitter:url" content="https://pathak22.github.io/hierarchical-imitation/" />
  <meta name="twitter:image" content="https://pathak22.github.io/hierarchical-imitation/resources/teaser.jpg" />
  <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" />
  <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" />
  <meta name="twitter:site" content="@pathak" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://pathak22.github.io/hierarchical-imitation/resources/teaser.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/eWBkDuNFEKA?rel=0&showinfo=0" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Third-Person Visual Imitation Learning via<br/> Decoupled Hierarchical Controller</span></center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://scholar.google.com/citations?user=RGiCLUgAAAAJ&hl=fr" target="_blank">Pratyusha Sharma</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~pathak/" target="_blank">Deepak Pathak</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a></span></center></td>
      <tr/>
      <tr>
        <td align=center width=200px>
        <center><span style="font-size:18px">MIT</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:18px">Facebook AI Research</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:18px">CMU</span></center></td>
      <tr/>
      </table>
      <table align=center width=700px>
          <tr>
            <td align=center width=700px><center><span style="font-size:22px"><a href="https://neurips.cc/Conferences/2019/AcceptedPapersInitial">NeurIPS 2019</a></span></center></td>
          <tr/>
      </table>
      <table align=center width=700px>
          <tr>
            <td align=center width=200px><center><span style="font-size:22px"><a href="https://drive.google.com/file/d/1I263unT0w7X3sB8mjVQVXW2OOp87_54A/view?usp=sharing">[Download Paper]</a></span></center></td>
            <td align=center width=200px><center><span style="font-size:22px"><a href='https://github.com/pathak22/hierarchical-imitation/'>[GitHub Code]</a></span></center></td>
          <tr/>
      </table><br/>

<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
      <iframe width="768" height="432" src="https://www.youtube.com/embed/eWBkDuNFEKA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align=center">
        We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling <b>what</b> to achieve (intended task) from <b>how</b> to perform it (controller). We propose a hierarchical setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box.
      </div>
      <br><hr>

      <center><h1>Decoupled Hierarchical Control</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        Our hierarchical approach consists of a goal generator that generates a goal visual state which is then used by the low-level controller as guidance to achieve a task. [Left] During training, the decoupled models are trained independently. The goal generator takes as input the human video frames <em>h<sub>t</sub></em> and <em>h<sub>t+k</sub></em> along with the observed robot state <em>s<sub>t</sub></em> to predict the visual goal state of the robot at <em>t+k</em>. The low level controller is trained using <em>s<sub>t</sub></em> ,<em>a<sub>t</sub></em> ,<em>s<sub>t+1</sub></em> triplets. [Right] At inference, the models are executed one after the other in a loop.
      </div><br/>
      <table align=center width=650px>
        <p style="margin-top:4px;"></p>
        <tr><td width=500px>
          <center><a href="resources/teaser.jpg"><img src = "resources/teaser.jpg"></img></a><br></center>
        </td></tr>
      </table>
      <br/><hr>

      <center id="sourceCode"><h1>Source Code and Environment</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
      We have released the PyTorch based implementation on the github page. Try our code!
      </div>
      <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:28px"><a href='https://github.com/pathak22/hierarchical-imitation/'>[GitHub]</a></span>
          </td>
        </tr>
      </table>
      <br><hr>

      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=250px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="resources/paper.pdf"><img style="height:150px" src="resources/thumbnail.jpeg"/></a>
        <center>
        <span style="font-size:20pt"><a href="resources/paper.pdf">[Paper]</a>&nbsp;
        <span style="font-size:20pt"><a href="resources/paper.pdf">[ArXiv]</a>
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Pratyusha Sharma, Deepak Pathak, Abhinav Gupta. <b>Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller<br/></b> In <i>NeurIPS</i> 2019.</span></p>
        <!-- <p style="margin-top:20px;"></p> -->
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('thirdPerson19_bib')" class="togglebib">[Bibtex]</a></span>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          <div class="paper" id="thirdPerson19_bib">
<pre xml:space="preserve">
@inproceedings{sharma19thirdperson,
    Author = {Sharma, Pratyusha and Pathak, Deepak
              and Gupta, Abhinav},
    Title = {Third-Person Visual Imitation Learning via
              Decoupled Hierarchical Controller},
    Booktitle = {NeurIPS},
    Year = {2019}
}</pre>
          </div>
          </td>
          </tr>
      </table>
    <br><hr>

    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      We would like to thank David Held, Xiaolong Wang, Aayush Bansal, and members of the CMU visual learning lab and Berkeley AI Research lab for fruitful discussions. The work was carried out when PS was at CMU and DP was at UC Berkeley. This work was supported by ONR MURI N000141612007 and ONR Young Investigator Award to AG. DP is supported by the Facebook graduate fellowship. <br>
      </left></td></tr>
    </table>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>
